Tue Jul  5 09:04:22 PDT 2016


Why is distributedR.ddR included in ddR? Shouldn't it be a separate
package? I guess it's here since it's an officially supported backend.
It's also very handy now for someone like me writing a new backend.

Reading through Michael's ddR documentation, think I'm getting the general
flavor.
<https://github.com/vertica/ddR/wiki/Design>

Looks like only the couple algorithms are reverse dependencies on CRAN
right now, so it should be ok to make changes. It's really worth it to get
the right abstraction.
<https://cran.r-project.org/web/packages/ddR/index.html>

I think that for this package to actually be successful and move the needle
it needs to be modular enough to handle all the ways package authors will
interact with distributed data in R.
The appeal to authors of packages implementing distributed algorithms is that they will run on
any system that follows this API. It's more flexibility all around.

## Mike Kane's talk

(including follow up discussion with Bryan)

Think about it as a grammar- composing operations on distributed objects.

Likes the distributed list, array, and dataframe. I agree.

All 3 distributed objects could be built on one simple object- the chunk

Separation of data and container API from execution- ie. use dplyr or base
R. Also may be possible to increase speed by writing specific code for each
backend.
 
Simpler and more general data and container API "API needs to be as simple
and stable as possible". Have to preserve backwards compatibility.

Cache optimization may be more important that data locality for `group_by`.

Serialization and multicore should be awesome and stable, because that's
what users will try first.

## references

Quick summaries of Mike's references

__cnidaria__

cnidaria: A Generative Communication Approach to Scalable, Distributed
Learning (2013)

> cnidaria is a computing framework for writing distributed data structures
> and algorithms

So it was basically the same idea as ddR.

Three types of problems:
1. Subset - (Subsample) Can do if iid. Linear regression has worked for me here
2. Divide and recombine - Same operation on many small subsets
3. Analysis Reduction - Goes beyond first two, ie. SVD on a large matrix

Idea is to have a configuration manager and all the other compute nodes
working decentralized peer-to-peer (P2P). Sounds difficult to actually
implement.

CAP theorem: consistency, availability, partition tolerance. Pick 2.

Don't really understand Figure 1. It appears to be just fetching a matrix
from one R process to another.

Seems related to functional programming.

The software uses `itertools`, which makes it even more related to the
functional ideas.

References: Matloff N (2013). “Software Alchemy: Turning Complex
Statistical Computations into Embarrass- ingly Parallel Ones.”
 
__Linda__ 

Generative communication in Linda, Gelernter (1985)

30 page classic paper - 3000 citations

Linda is a distributed language. Basic idea is Tuple space with operations
`read, in, out` on tuples.

This is certainly further than we want to go- it's writing the whole
system. We just want an abstraction.

## Kicking the tires

```
> source('examples/dkmeans-example.R')

Welcome to 'ddR' (Distributed Data-structures in R)!
For more information, visit: https://github.com/vertica/ddR

Attaching package: ‘ddR’

The following objects are masked from ‘package:base’:

    cbind, rbind

Generating data with rows= 1e+06  and cols= 100 
training dkmeans model on distributed data:  11.869 
training normal kmeans model on centralized data:  16.384 
Warning messages:
1: did not converge in 10 iterations 
2: did not converge in 10 iterations
```

TODO: to set a seed or modify these so the example scripts work.

After some modification:

```
> source('examples/dkmeans-example.R')
Generating 100000 x 100 matrix for clustering with 10 means
training dkmeans model on distributed data
14.975
training normal kmeans model on centralized data
10.34
Warning messages:
1: did not converge in 100 iterations
2: empty cluster: try a better set of initial centers
```

So what's up with `cbind` and `rbind`? They're being exported.

Both sparse and dense `darray`s are there in `dobject.R`.

TODO: improve print methods with distributed metadata.
Print method- it can sometimes be very handy to see the first couple
rows. But this would preclude lazy evaluation.

Right now `dmapply` is not lazy. It evaluates as soon as it receives the
command. Guessing it would be better if it deferred computation as long as
possible like dask and itertools.

- I would have expected a different split based on the data sizes:
```
> dl2 = dlist(1:10, letters, runif(5), nparts=2)
> dl2

ddR Distributed Object
Type: dlist
# of partitions: 2
Partitions per dimension: 2x1
Partition sizes: [2], [1]
Length: 3
Backend: parallel
```

The docs seem to preclude the use of arrays with dimension higher than 2.

So `collect` brings it back into memory of the local process.

### Question

So how exactly would I write a group by operation? (Ryan Hafen's
common use case) This is something encountered very often in data analysis.
Split a big table on some factor, load one observation into memory and
figure out what you're trying to do, then apply that to the big table.
ddR doesn't have any facility to do this, but it's a natural way to think
about the problem.

On a related note, it seems super weird to me to chunk a table like this:
```
a b c d
. .|. .
-------
. .|. .
```
Because this doesn't correspond to a logical or physical partitioning. So
what's the goal of ddR? Do the chunks actually correspond to either of
these? Or do we just know that there are chunks?

All the focus is on dmapply and dlapply. What about a plain old Map?

Uses S4 classes? But seems to mention S3 methods also?

TODO: Add ability to select ith row or jth column. `da[2, ]` which
currently throws an error.

`repartition` function seems like it might be more expensive than it needs
to be.

The big thing I need to be thinking about is how to write a driver for a
new backend.

What exactly would it mean to have a serial backend like Bryan Lewis was
talking about? What about a memory mapped backend?

What would be a compelling use case? What about algorithms that involved
diagonal block matrices. Those are sparse and proper representation of the
structure will accelerate any computation. Can ddR naturally build and
operate on those?

So I believe it's possible to write backend specific code, ie. `colMeans` for a
particular class. How exactly is this done? Ah, see `colnames` in 
`distributedR.ddR/R/ops.R`.


