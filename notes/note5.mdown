# Installing Local Spark

You don't need hadoop for standalone mode, but it's useful to have hdfs.

This command had me first install Java.
```
$ brew install hadoop
```

Download desired spark image - I'm using the Preview 2.0.0 release
<http://spark.apache.org/downloads.html>

Navigate into image and then:
```
./build/mvn -Pyarn -Phadoop-2.7 -Dhadoop.version=2.7.2 -DskipTests clean
package
```

This takes about 10 minutes.

Now testing R

```
$ ./R/run-tests.sh 
Error in library(SparkR) : there is no package called ‘SparkR’
Execution halted
```

So we'll need to install SparkR here.

```
R CMD INSTALL R/pkg/
```

Works fine.

```
./bin/sparkR
```

Now this looks just like a normal R interpreter.
Following this tutorial:
<http://spark.apache.org/docs/2.0.0-preview/sparkr.html>

Seems like the basics work as expected.

Moving onto the `sparkapi` package. Looks like I need hive for this to
work. hive installs with brew.

```
hive> show databases;
OK
default
Time taken: 0.215 seconds, Fetched: 1 row(s)
hive> USE default;
OK
Time taken: 0.028 seconds
hive> show tables;
OK
Time taken: 0.035 seconds
hive> quit;
campus-030-003 ~/dev/sparkapi $ 
```

Ok, I still get the same errors

```
> library(sparkapi)
> sc <- start_shell(master = "local")
Error: java.lang.IllegalArgumentException: Unable to instantiate SparkSession with Hive support because Hive classes are not found.
        at org.apache.spark.sql.SparkSession$Builder.enableHiveSupport(SparkSession.scala:731)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.spark.api.r.RBackendHandler.handleMethodCall(RBackendHandler.scala:141)
        at org.apache.spark.api.r.RBackendHandler.channelRead0(RBackendHandler.scala:86)
        at org.apache.spark.api.r.RBackendHandler.channelRead0(RBackendHandler.scala:38)
        at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
        at io.netty.channel.AbstractCh
```

First I better check if I can load data into HDFS and hive and get started with
that. I'll use the `Orange` data sets in R.

Looks like the HDFS setup is a little more involved:
<http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/SingleCluster.html>

Not making much headway with this. Is it possible to skip it?

Seems like I need a Hive metastore and all kinds of other configuration.

But I didn't build Spark with Hive support. I'll try to go back and do
that.

```
./build/mvn -Pyarn -Phadoop-2.7 -Dhadoop.version=2.7.2 -Phive -Phive-thriftserver -DskipTests clean package

...

[ERROR] Failed to execute goal
net.alchim31.maven:scala-maven-plugin:3.2.2:compile (scala-compile-first)
on project spark-tags_2.11: Execution scala-compile-first of goal net.alc
him31.maven:scala-maven-plugin:3.2.2:compile failed. CompileFailed -> [Help
1]
```

Compilation errors. Maybe I can start with a fresh build.
First set JAVA HOME

<http://stackoverflow.com/questions/1348842/what-should-i-set-java-home-to-on-osx>

```
$ ./dev/make-distribution.sh --name clarkspark --tgz -Psparkr -Phadoop-2.7 -Phive -Phive-thriftserver -Pyarn
```

Same errors. Maybe I should delete it all and start again. Feel like this
is wasting a lot of time. Can I just start with the databricks product
maybe?

Trying the community edition. <https://community.cloud.databricks.com>
Let's see if it allows me to get in and try Rstudio's `sparkapi`.

Looks like the only access is through the notebook. Not ideal for trying to
build this stuff.

Looking at 'Intro to big data' Python course
RDD in Spark - resilient distributed dataset. Immutable data structure,
seems to act somewhat like an iterator in Python.
