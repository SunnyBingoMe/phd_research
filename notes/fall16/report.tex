% A simple template for LaTeX documents
% 
% To produce pdf run:
%   $ pdflatex paper.tex 
%

\documentclass[12pt]{article}

% Begin paragraphs with new line
\usepackage{parskip}  

% Change margin size
\usepackage[margin=1in]{geometry}   

% Graphics Example:  (PDF's make for good plots)
\usepackage{graphicx}               
% \centerline{\includegraphics{figure.pdf}}

% Allows hyperlinks
\usepackage{hyperref}

% Blocks of code
\usepackage{listings}
\lstset{basicstyle=\ttfamily, title=\lstname}
% Insert code like this. replace `plot.R` with file name.
% \lstinputlisting{plot.R}

% Monospaced fonts
%\usepackage{inconsolata}
% GNU \texttt{make} is a nice tool.

% Supports proof environment
\usepackage{amsthm}

% Allows writing \implies and align*
\usepackage{amsmath}

% Allows mathbb{R}
\usepackage{amsfonts}

% Numbers in scientific notation
% \usepackage{siunitx}

% Use tables generated by pandas
\usepackage{booktabs}

% norm and infinity norm
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\inorm}[1]{\left\lVert#1\right\rVert_\infty}

% Statistics essentials
\newcommand{\iid}{\text{ iid }}
\newcommand{\Exp}{\operatorname{E}}
\newcommand{\Var}{\operatorname{Var}}
\newcommand{\Cov}{\operatorname{Cov}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\title{Fall 2016 report}
\date{December 9, 2016}
\author{Clark Fitzgerald}
\maketitle

This is the report for Professor Duncan Temple Lang for 8 units of
credit in Fall 2016.  The tentative plan is to take the qualifying exam in
Spring 2017.

\begin{textbf}
    Task: Describe 3 or 4 things to focus on, why they're interesting, what are the
challenges, and who cares?
\end{textbf}


\section{Code Generation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The first task here is to create software that generates R bindings for an
existing C and C++ libraries. 
It should be understood that this task will be used as a base for
further work, ie. detecting and using parallelism.

This would be an extension of Duncan's work with the RCIndex \cite{R-RCIndex} and
RCodegen \cite{R-RCodegen} packages, along with other prior work.

This is useful because writing these bindings by hand
has several problems: 
Writing by hand is error prone.
It's tedious to write 1000's of wrappers.
Hard to update wrapping package if software version is different.

This is interesting because it allows one to very quickly add new
capabilities to R as a system. The new capabilities then facilitate rapid
prototyping and novel types of data analysis, using R as a "glue language".

Duncan describes further applications and motivations in the paper in
the RCIndex pacakge repository \cite{R-RCIndex}.

\subsection{Challenges}

This will require me to spend significant time building expertise in C/C++.

Memory management and garbage collection is potentially difficult. A C routine might allocate
memory that R knows nothing about. Then how is that memory protected and freed? A
possible solution to this problem is to look in the body of the code
itself, which RCIndex doesn't yet do.

\subsection{Related Work}

Many packages on CRAN simply wrap existing C/C++ libraries. When done
thoughtfully this can be very useful. Ie. the programmer maps the C/C++ API
into idiomatic data structures and calls them in the new language, in this case
R. 
The intent with the code generation is to \emph{augment} rather than
\emph{replace} such libraries. 

The approach in RCIndex differs from existing software like SWIG
\cite{swig} and Rcpp \cite{R-Rcpp} because it hooks directly into the clang
compiler rather than processing the text itself. This means we have a code
analysis tool that will be more robust and consistent with behavior of a compiler. 

\subsection{Applications}

The mature C++ computer vision library openCV
\cite{opencv_library} is an example of capabilities that we would like to
access from within R. It's possible to directly write computer vision
algorithms in R, but this is a huge amount of duplicated work if a mature
library already exists.

Another application is cutting edge specialized machine learning code like
Professor Cho-Jui Hsieh's Hogwild++ \cite{zhang2016hogwild}.

\section{Chunking for Parallelism}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Idea: Users write normal R code which operates on larger than
memory data in a single machine. This is done through analysis of the
user's code.

A well known technique for parallel computation is to split data into
chunks and compute on each chunk simultaneously. This approach 
is useful on large data. Multiplying large block matrices is an example of this.

Many existing software packages for parallel computation requires one to specify the
chunk size. Examples include ddR \cite{R-ddR} and partools
\cite{R-partools} in R and dask in Python.
Yet chunk size strongly impacts
performance. If it's too small then there will be excessive overhead. If
it's too large then it won't fit into memory. In both cases performance
will suffer. But how much will it suffer?

Matloff presents compelling reasons to use a chunk based
approach for statistical applications \cite{matloff2014software}.

Apache Spark does chunking transparently to the user.


Question: How much does chunk size affect performance? How do we choose a
really good one? How much do we lose by choosing bad ones?

Python's dask library infers appropriate block size for
\href{https://github.com/dask/dask/pull/1328}{reading CSV files}.

R's bigmemory package uses C++ and a memory mapped file which allows shared
memory access. \cite{kane2010bigmemory} It's a tool for representing large
data sets, and the package vignette mentions that chunking can and should
be used together when possible.

Idea: for a given set of hardware and a given task graph in a given
computational model there may be
an optimal chunk size, or a range of chunk sizes with similar performance.
We'd like to automatically figure that out so the user doesn't have to
specify. This could potentially be posed as an optimization problem.

I'd expect that a dynamic language calling vectorized C code has some set
of performance characteristics, while standalone C++ code is quite
different.
Extension: What chunking is optimal for GPU?

Allowing \texttt{rechunk()} operations in the middle of the task graph adds
much more flexibility and complexity. But it might be useful.

Need to be a little careful because the chunking scheme defines the dask
task graph. Then the semantics are more general than the dask graph. It's
really similar to optimization.

\subsection{Applications}

When exactly is this useful? Optimization probably works best when we know what we want
to do- ie. we did the exploratory data analysis on a small subset and we're
just extending it to the full amount.

Here's the simplest application.
Consider a CSV file too large for memory, like the NYC taxi data. 
That will
be read in chunks. Suppose the data is a table like 

Suppose we're doing something simple, say computing a
function that's vectorized, ie. it operates on each row. Say we want to
know the 

\section{Sample Problem}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{table}
    \begin{tabular}{| l | l | l | l | l |}
    \hline
        Date & Time & Freeway & Postmile & Flow \\ \hline
        1 Apr & 11:00 & I80E & 65 & 105 \\ 
        1 Apr & 11:05 & I80E & 65 & 87 \\ 
        $\dots$ & & & & \\ \hline
    \end{tabular}
    \caption{Sample traffic data}
    \label{table:traffic}
\end{table}

Suppose a directory contains 365 files of traffic data, one for every day.
Data is characterized by one observation (say vehicular flow)
indexed by (Date, Time, Freeway, Postmile). So the data looks like table
\ref{table:traffic}.

For each freeway the analyst wants to compute this operation expressed in SQL
\texttt{
    SELECT Time, Postmile, mean(flow) AS meanflow FROM traffic GROUP BY Time, Postmile
}
This amounts to splitting the data a

\begin{enumerate}
    \item 
\end{enumerate}

\section{Parallelism through pure functions}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Programs using R's \texttt{Apply} family of functions together with pure
functions\footnote{Pure functions are those that depend only on inputs and
have no side effects \cite{wiki:purefunc}.} can be easily parallelized
\cite{bohringer2013dynamic}. Then why not provide a mechanism for users to
declare pure functions?

Consider an R script with the following form:
\begin{verbatim}
data = read.table("data.txt")

y = f(data)
fdata = lapply(g, data)
...

write.table(result, "result.txt")
\end{verbatim}

\texttt{read.table()} and \texttt{write.table()} are not pure functions,
but suppose that everything consists of calls to pure functions.

If we force users to declare and use pure functions
 then detecting and using parallelism at the user
level can be simplified.  \subsection{Related Work}

\subsection{Related Work}

This idea came from BÃ¶hringer, who describes a relevant algorithm and
R implementation in a 2013 paper "Dynamic Parallelization of R Functions"
\cite{bohringer2013dynamic}.

\subsection{Challenges}

User settings could affect results in subtle ways, for example if a 

Functions using random sampling require careful handling.

\section{Statistically Efficient File Format}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The high level goal here is to demonstrate a system with associated
techniques that works well on data larger than memory.

Can an existing file format be combined with a chunk based parallel
computational strategy to produce a system that allows users to write high
level R code that operates on 

The file format should
have the following properties:

\begin{enumerate}
    \item{fast} Fast read / write operations, hopefully in parallel
    \item{typed} Integers are read in as integers, floats are floats, etc.
        Factors would be nice to have.
    \item{efficient} File size no larger than it needs to be, so store as
        binary rather than text
    \item{portable} Works in R, Python, Julia, etc.
    \item{iterable} Can take the first $k$ rows and process them
        sequentially
    \item{computationally efficient} Works well for parallel block
        operations with R / Numpy vectorized computational model.
    \item{statistically efficient} Can be used to seamlessly produce and
        compute on iid blocks as in Matloff's
        software alchemy \cite{matloff2014software}.
\end{enumerate}

The last two items are the novel ones. By using a file format
that facilitates a chunk based computational strategy it may be possible to
realize large gains in performance.

It would also be possible to simply chunk R objects then serialize 
them to disk. This would be faster than using 

\subsection{Related Work}

Since 1998 HDF5 has provided most of the desired properties above
\cite{hdf5}, including
\href{https://support.hdfgroup.org/HDF5/doc/Advanced/Chunking/}{sophisticated
chunking} and a flexible
\href{https://support.hdfgroup.org/HDF5/doc1.6/UG/11_Datatypes.html}{type system}.
Mario Annau's recent work on R's \href{https://github.com/mannau/h5}{h5
package} provides R S4 methods for HDF5 data structures.

This year (2016) Wes Mckinney and Hadley Wickham began development on Feather and
Apache Arrow. From the documentation for Feather, this "provides binary columnar
serialization for data frames." It uses a common C++ implementation with
Python and R bindings to support basic types such as numeric, logical,
dates, and strings. NA's for all types are also supported. So this may also
be a candidate to build from.

R's iotools package provides exciting high performance tools for reading
and writing standard text data, ie. CSV files \cite{arnold2015iotools}.
Pipeline parallelism is explicitly supported. The package is used to
integrate R with Hadoop Streaming and Spark to process terabyte to petabyte
scale data.

\subsection{Challenges}

Integrating such a system with other data sources

\subsection{Applications}

Databases and programming environments store tables in an efficient
structured format. Yet this structure is often lost when the data is
written as a flat text file. Consequently much time in data analysis is
spent loading, cleaning, and inferring the structure of data.


\section{Transportation Data}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In an effort to keep research relevant, it would be good to keep
applications and use cases in mind. Physical transportation systems provide
rich sources of open data. An example is Caltrans PeMS (Performance
Management System), which integrates various data related to vehicle
traffic on freeways in California \cite{jia2001pems}.

PeMS alone provides 10 TB of data, which is more than enough to create
computationally demanding problems.

\subsection{Problem}

Use the PeMS data to quantify highway incidents.

The following task is computationally challenging and has a statistical
flavor.


An interesting task is to implement it with several systems and see which
perform well and why? The systems to consider are distributed R, Python's
dask, Spark SQL, Postgres. Maybe SAS, Julia, C++, different Spark. The solutions can be compared in terms
of:
\begin{enumerate}
    \item{Complexity} lines of code, background knowledge required.
        Maintainability and standardization of organization, ie. software
        engineering.
        Expect Postgres SQL < Spark < dask < parallel R < C++.
    \item{Run Time} To run on 1 year of data.
        Expect Postgres SQL > Spark > dask > C++. Not sure where parallel R
        would fit in.
    \item{Flexibility} How easy is it to do something beyond what base SQL
        allows, for example? Can we inspect intermediate variables, debug,
        and make plots as we go?
    \item{Scalability} Will the software scale up to 1 TB, for example?
\end{enumerate}

Title: Evaluation of state of the art technologies for processing datasets
on the order of 10 GB. This could result in recommendations to researchers
and industry on which systems to consider for these types of tasks.

This comparison between technologies could be improved by having a couple
more practical data analysis examples.

Related article compares Python threading and multiprocessing
technologies \cite{malakhov2016composable}.

Why is this interesting? Most software comparisons are biased, ie. the
comparisons are presented by a developer creating new software. For that
new software to be adopted it should be presented in a favorable light. 
This is similar to the statistician hunting for new datasets which show the
advantages of their proposed method. How many datasets are discarded where
the method fails?
Further, each of these technologies takes time to learn and understand-
few people have reason to take the time to learn this.

Academics likely lag far behind industry with these

Questions to answer:
\begin{enumerate}
    \item How many traffic events occur which have no associated incident
        data? And what sort of events were they probably?
    \item What is the impact of an event of type X on a given section of
        highway? Something along the lines of: When traffic flow is 1500
        veh per lane per hour in a two lane freeway a collision involving
        exactly two vehicles typically creates congestion lasting 10-15
        minutes which propagates back 2-3 miles. 
    \item How can we model the distribution of traffic incidents, ie.
        Poisson with some parameters.
\end{enumerate}



A different approach would be to treat the occupancy data as \emph{images}.
We can use opencv to do this. For each freeway we can take the following
steps:
\begin{enumerate}
    \item Use R to read the raw file and convert to an image with many missing
        values. This means expansion to a uniformly spaced grid.
    \item Interpolate missing values. 
    \item Threshold the image, so for a pixel with density $\rho >
        \rho_0$
        we define it as high density, otherwise low density. We'll
        have to experiment to see what the appropriate value of $\rho_0$ is
        but I suspect around 0.3.
    \item Once this has been done for every day we can compute an average
        thresholded image showing the areas of high density. This will show
        recurring bottlenecks. May need to threshold this also.
    \item Compare each day with the average. The difference will show the
        unusual patterns that occurred on just one day. Might have to do
        some denoising here- eliminate points that are not part of a 
        cluster.
\end{enumerate}

Once we have the denoised difference we can do the following:
\begin{enumerate}
    \item (Optional) Detect shapes that are flat on top, since this is the
        distinguishing feature of a bottleneck.
    \item Compute centroid, bounding boxes, and area which will quantify the impact
        in terms of space and time.
    \item Join these features to incident data. This likely will require some text
        analysis.
\end{enumerate}


\subsection{Applications}

Self driving cars are poised for widespread adoption which will create
dramatic change over the next several decades. This will generate rich
sources of data which can be used to improve the technology and the overall
transportation system performance.\cite{swan2015connected}
As such, there will be many further research applications.

Of particular interest are problems related to the overall system. Every
vehicle produces large data streams and there are hundreds of
thousands of vehicles on the road in an urban area at any given time.
Then data processing tools need to be flexible, powerful, and scalable to
handle new problems.

\section{What I've learned}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Research can be ambiguous. The line between engineering / implementation
and research is not totally clear.

Research is about developing new ideas. Engineering is about making
things that will be adopted because they work and are useful. Ideally these
two activities overlap, but not every research topic will lead to useful
software.

As an inexperienced graduate student it's difficult to know whether a topic
or approach is new or merits further investigation.

\bibliographystyle{plain}
\bibliography{citations} 

\section{Maybe include somewhere?}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


Common statistical methods rely on iid samples. Consider the problem of
taking a random sample from

Matloff's partools package \cite{R-partools} somewhat resembles ddR and
dask. cool. I wonder what the underlying files are?
\texttt{readnscramble()} looks useful for getting a sample of data.


\end{document}
