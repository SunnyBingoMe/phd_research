# Design Document

A ddR backend should implement the following methods:

## object.R

__initialize__ Function `new` calls this to create a new parallel object

__combine__ 

```
a = dlist(1:5, 2)
b = dlist(1:10)

combine(ddR:::ddR.env$driver, list(a, b))

Error in if (!is.dlist(dobj)) ans <- t(ans) :
  missing value where TRUE/FALSE needed
```

__get_parts__ 

Only used in one place:

```
~/dev/ddR/ddR/R $ grep "get_parts(" *
dobject.R:  partitions <- get_parts(dobj, index)
```

TODO: understand `combine` and `getparts`. I'm not having any luck using
these with default parallel backend.

__do_collect__ Move from storage to local list, matrix, or data.frame in
calling context.

## driver.R

__init__ Called when the backend driver is initialized.

__shutdown__ Called when the backend driver is shutdown.

__do_dmapply__ Backend-specific dmapply logic.


# Spark 

We need to execute user defined functions on local dataframes, matrices, and lists.
Dataframes and matrices can be partitioned by both rows and columns. The
underlying idea of Spark dataframes is RDD's consisting of rows, which is
not compatible with partitioning the columns.

In order to use Spark in this more general way the idea is to have it
simply act as a key value pair which stores binary R objects and can run R
code:

row |   column  |   BLOB 
------------------------
1   |   1       |   XXX
1   |   2       |   XXX
... |   ...     |   ...

Here BLOB is an RDS object.

## Dependencies

There are two options for packages that could be depended on to
implement Spark as a backend

### sparkapi

With this lower level approach we can store the distributed object directly
in a [Pair
RDD](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaPairRDD)

__Pros__

__Cons__

### SparkR
